{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9533f7",
   "metadata": {},
   "source": [
    "# The world is cruel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c89063a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import distance\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from thefuzz import fuzz\n",
    "import spacy\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from joblib import dump, load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ad7d7",
   "metadata": {},
   "source": [
    "# To predict in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67962bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_features(question_one, quesion_two):\n",
    "    SAFE_DIV = 0.0001\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    simple_features = [0.0] * 10\n",
    "\n",
    "    question_one_tokens = question_one.split()\n",
    "    quesion_two_tokens = quesion_two.split()\n",
    "\n",
    "    if len(question_one_tokens) == 0 or len(quesion_two_tokens) == 0:\n",
    "        return simple_features\n",
    "\n",
    "    question_one_words = set([word for word in question_one_tokens if word not in STOP_WORDS])\n",
    "    quesion_two_words = set([word for word in quesion_two_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    question_one_stops = set([word for word in question_one_tokens if word in STOP_WORDS])\n",
    "    quesion_two_stops = set([word for word in quesion_two_tokens if word in STOP_WORDS])\n",
    "\n",
    "    common_word_count = len(question_one_words.intersection(quesion_two_words))\n",
    "    common_stop_count = len(question_one_stops.intersection(quesion_two_stops))\n",
    "    common_token_count = len(set(question_one_tokens).intersection(set(quesion_two_tokens)))\n",
    "\n",
    "    simple_features[0] = common_word_count / (min(len(question_one_words), len(quesion_two_words)) + SAFE_DIV)\n",
    "    simple_features[1] = common_word_count / (max(len(question_one_words), len(quesion_two_words)) + SAFE_DIV)\n",
    "    simple_features[2] = common_stop_count / (min(len(question_one_stops), len(quesion_two_stops)) + SAFE_DIV)\n",
    "    simple_features[3] = common_stop_count / (max(len(question_one_stops), len(quesion_two_stops)) + SAFE_DIV)\n",
    "    simple_features[4] = common_token_count / (min(len(question_one_tokens), len(quesion_two_tokens)) + SAFE_DIV)\n",
    "    simple_features[5] = common_token_count / (max(len(question_one_tokens), len(quesion_two_tokens)) + SAFE_DIV)\n",
    "    simple_features[6] = int(question_one_tokens[-1] == quesion_two_tokens[-1])\n",
    "    simple_features[7] = int(question_one_tokens[0] == quesion_two_tokens[0])\n",
    "    simple_features[8] = abs(len(question_one_tokens) - len(quesion_two_tokens))\n",
    "    simple_features[9] = (len(question_one_tokens) + len(quesion_two_tokens)) / 2\n",
    "\n",
    "    return simple_features\n",
    "\n",
    "\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "\n",
    "\n",
    "def get_vectors_predict(question, tfidf_model, nlp_model):  # Step - 3.2\n",
    "    doc1 = nlp_model(question)\n",
    "    tfid_vector = np.zeros([len(doc1), len(doc1[0].vector)])\n",
    "\n",
    "    for word in doc1:\n",
    "        vector = word.vector\n",
    "\n",
    "        try:\n",
    "            idf = tfidf_model[str(word)]\n",
    "        except:\n",
    "            idf = 0\n",
    "\n",
    "        tfid_vector += vector * idf\n",
    "\n",
    "    tfid_vector = tfid_vector.mean(axis=0)\n",
    "\n",
    "    return list(tfid_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec9c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(question):\n",
    "    question = str(question).lower()\n",
    "    question = question.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "\n",
    "    question = re.sub(r\"([0-9]+)000000\", r\"\\1m\", question)  # could be done with replce\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    pattern = re.compile('\\W')\n",
    "\n",
    "    if type(question) == type(''):\n",
    "        question = porter.stem(question)  # programming -> program\n",
    "\n",
    "        example1 = BeautifulSoup(question)\n",
    "        question = example1.get_text()\n",
    "\n",
    "    if type(question) == type(''):\n",
    "        question = re.sub(pattern, ' ', question)\n",
    "\n",
    "    return question\n",
    "\n",
    "\n",
    "def get_features_predict(question_one, quesion_two):  # Step - 1\n",
    "    question_one = preprocess(question_one)\n",
    "    quesion_two = preprocess(quesion_two)\n",
    "\n",
    "    features = get_simple_features(question_one, quesion_two)\n",
    "\n",
    "    features.append(fuzz.token_set_ratio(question_one, quesion_two))\n",
    "    features.append(fuzz.token_sort_ratio(question_one, quesion_two))\n",
    "    features.append(fuzz.QRatio(question_one, quesion_two))\n",
    "    features.append(fuzz.partial_ratio(question_one, quesion_two))\n",
    "    \n",
    "    features.append(get_longest_substr_ratio(question_one, quesion_two))\n",
    "    \n",
    "    features.append(1)\n",
    "    features.append(1)    \n",
    "    features.append(len(question_one))\n",
    "    features.append(len(quesion_two))\n",
    "    features.append(len(question_one.split(\" \")))\n",
    "    features.append(len(quesion_two.split(\" \")))\n",
    "    \n",
    "    word_Common = 1.0 * len(set(question_one.split(\" \")).intersection(set(quesion_two.split(\" \"))))\n",
    "    word_Total = 1.0 * (len(question_one.split(\" \")) + len(quesion_two.split(\" \")))\n",
    "    \n",
    "    features.append(word_Common)    \n",
    "    features.append(word_Total)    \n",
    "    features.append(word_Common / word_Total) \n",
    "    \n",
    "    features.append(2)    \n",
    "    features.append(0)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def transform(question_one, quesion_two):\n",
    "    features = get_features_predict(question_one, quesion_two)\n",
    "\n",
    "    with open('Models/tfid_model.p', 'rb') as fp:\n",
    "        tfid = pickle.load(fp)\n",
    "\n",
    "    nlp_model = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    question_one_vector = get_vectors_predict(question_one, tfid, nlp_model)\n",
    "    question_two_vector = get_vectors_predict(quesion_two, tfid, nlp_model)\n",
    "\n",
    "    to_test = [features + question_one_vector + question_two_vector]\n",
    "    \n",
    "    return to_test\n",
    "\n",
    "\n",
    "def load_models():\n",
    "    lr_cal_model = load(\"Models/Logistic_Calibrated_Quora.pkl\")\n",
    "\n",
    "    svm_cal_model = load(\"Models/SVM_Calibrated_Quora.pkl\")\n",
    "\n",
    "    keys = pd.read_csv(\"Models/Boost_Keys.csv\", encoding='latin-1')\n",
    "    keys.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    keys = keys.T.to_numpy()[0]\n",
    "\n",
    "    bst = xgb.Booster()\n",
    "    bst.load_model('Models/Boost_F_Quora.json')\n",
    "\n",
    "    stack_model = load(\"Models/Stack_F_Quora.pkl\")\n",
    "    \n",
    "    return (lr_cal_model, svm_cal_model, stack_model, bst, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75c5fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_real_time(question_one, quesion_two):\n",
    "    to_test = transform(question_one, quesion_two)\n",
    "    \n",
    "    lr_cal_model, svm_cal_model, stack_model, boost_model, keys = load_models()\n",
    "\n",
    "    boost_yp = xgb.DMatrix(pd.DataFrame(data=np.array(to_test[0])[:, np.newaxis].T, columns=keys))\n",
    "\n",
    "    p_boost = boost_model.predict(boost_yp)\n",
    "    p_lr = lr_cal_model.predict_proba(to_test)\n",
    "    p_svm = svm_cal_model.predict_proba(to_test)\n",
    "    p_stack = stack_model.predict_proba(to_test)\n",
    "\n",
    "    print(\"Logistic:\", 1 - p_lr[0][0], \"\\nSVM:\", 1 - p_svm[0][0],\n",
    "          \"\\nBoosting: \", p_boost[0], \"\\nStacking: \", 1 - p_stack[0][0])\n",
    "\n",
    "    print(\"\\nMean:\", (p_boost[0] + 1-p_lr[0][0] + 1-p_svm[0][0] + 1-p_stack[0][0]) / 4)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "085781bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: 0.3214695739700255 \n",
      "SVM: 0.46391128045886343 \n",
      "Boosting:  0.26260194 \n",
      "Stacking:  0.41151093990545595\n",
      "\n",
      "Mean: 0.36487343403957606\n"
     ]
    }
   ],
   "source": [
    "question_one = \"Who am I?\"\n",
    "quesion_two = \"What am I?\"\n",
    "\n",
    "predict_real_time(question_one, quesion_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400da52c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
